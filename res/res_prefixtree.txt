Obtaining file:///lustre/fswork/projects/rech/dsv/ufy16sp/Stage_M2/llama3
Installing collected packages: llama3
  Attempting uninstall: llama3
    Found existing installation: llama3 0.0.1
    Uninstalling llama3-0.0.1:
      Successfully uninstalled llama3-0.0.1
  Running setup.py develop for llama3
Successfully installed llama3-0.0.1
-----------------------------------------
Mémoire RAM disponible avant exécution :
               total        used        free      shared  buff/cache   available
Mem:           502Gi        29Gi       457Gi       424Mi        26Gi       472Gi
Swap:             0B          0B          0B
-----------------------------------------
Exécution de Test_Generation.py sans argument d'entrée avec torchrun...
ModelArgs with use_scaled_rope loaded!
Chargement du Trie depuis Llama-3.2-3B/original/tokenized_trie.json
Loading Tokenized_Trie from file
Tokenized_Trie loaded successfully
Trie chargé en 230.26s.
Total GPUs available: 1
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
Modèle chargé en 14.50s.
Starting generation: 1 prompt, max_gen_len=77
min_prompt_len=97, total_len=174
Initial tokens:
tensor([[128000,   2374,     25,    445,      6,  78191,   1826,    653,   6335,
           7010,    514,   1161,      8,  96839,   1161,      8,    384,   3415,
          64591,   1161,      8,    326,  86509,     13,   7695,  42182,  75871,
            265,    409,  85722,  51625,   1082,     11,  64694,   1880,   8791,
            458,     11,    665,  81994,  66262,    951,  44827,  62623,    288,
           1880,    665,   5201,    519,   3625,  11470,  46681,  90452,    308,
          59858,    264,    934,  31109,  86806,  62163,    409,    326,  86509,
           1880,    326,      6,  78191,  42182,  75871,    265,   3869,  20662,
          62163,  15878,   1153,    261,    409,   4860,  82113,  42625,  17724,
            627,  32772,     25,  71083,  41091,   6316,   1421,   6672,    409,
            296,   4776,    361,    627,    197,  72803,     25,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,
             -1,     -1,     -1]])
Generating position 97
Prefix tokens: [128000]
Sampled next token with temperature=0.6: tensor([[28615]])
EOS reached: [False]
Generating position 98
Prefix tokens: [128000, 28615]
Sampled next token with temperature=0.6: tensor([[3457]])
EOS reached: [False]
Generating position 99
Prefix tokens: [128000, 28615, 3457]
Sampled next token with temperature=0.6: tensor([[3625]])
EOS reached: [False]
Generating position 100
Prefix tokens: [128000, 28615, 3457, 3625]
Sampled next token with temperature=0.6: tensor([[1206]])
EOS reached: [False]
Generating position 101
Prefix tokens: [128000, 28615, 3457, 3625, 1206]
Sampled next token with temperature=0.6: tensor([[5512]])
EOS reached: [False]
Generating position 102
Prefix tokens: [128000, 28615, 3457, 3625, 1206, 5512]
Sampled next token with temperature=0.6: tensor([[128001]])
EOS reached: [True]
Sequence reached EOS — stopping early.
Sequence stopped at token 128001 (index 5)
Generation complete.
	Input: System: L'assistant est un expert dans le(s) domaine(s) evoqué(s) l'utilisateur. Il doit répondre de manière précise, concise et utile, en fournissant des informations pertinentes et en respectant les instructions données.Il n'y a qu'une seule demande de l'utilisateur et l'assistant doit répondre à cette demande sans poser de questions supplémentaires.
	User: Donne moi une recette de meringue.
	Assistant:
	Voici les clés
{'generation': 'Voici les clés'}
